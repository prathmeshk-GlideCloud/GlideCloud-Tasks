import os
import requests
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL")


def generate_explanation(
    itinerary_summary: str,
    retrieved_context: str,
    constraints_summary: str
) -> str:
    """
    Uses a local Ollama LLM to generate a human-readable explanation
    for a deterministic travel itinerary.
    """

    prompt = f"""
You are explaining a travel itinerary generated by a deterministic constraint-based planner.

Constraints:
{constraints_summary}

Itinerary:
{itinerary_summary}

POI context:
{retrieved_context}

Explain clearly:
1. Why these places were chosen
2. How constraints were satisfied
3. What trade-offs were made

Important rules:
- Do NOT change the itinerary
- Do NOT suggest new places
- Do NOT optimize anything
"""

    response = requests.post(
        f"{OLLAMA_BASE_URL}/api/generate",
        json={
            "model": OLLAMA_MODEL,
            "prompt": prompt,
            "stream": False
        },
        timeout=60
    )

    response.raise_for_status()

    return response.json()["response"]
